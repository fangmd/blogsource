---
title: Scrapy 
date: 2016-12-23 13:18:12
tags: [Python, Crawler, Scrapy]
category: Python

---


# 理解

Scrapy 爬虫主要组件

## Spider 类
定义了爬虫的目标 URL

Spider 类中有个 `def parse(self, response):` 方法，这个方法中解析请求到的网页内容

最简单的操作： 把网页全部保存下来

```python
    def parse(self, response):
        filename = response.url.split("/")[-2] + ".html"
        with open(filename, "wb") as f:
            f.write(response.body)
```

### 启动一个爬虫

```
scrapy crawl <spider.name>
```

## Selector

通常使用的时候从 response 对象中获取 `response.selector`

在 Spider 类中的 `parse` 方法中使用 `Selector` 提取数据并保存到创建的 Item 中。


## Items
用于保存爬取的数据的对象

## Item Pipeline

作用：对 Item 做进一步处理

- 清理  HTML 数据
- 验证爬取的数据 （检查 item 包含某些字段）
- 查重 （并丢弃）
- 将爬取结果保存到数据库中

Pipeline 继承自 object， 内部需要定义 `def process_item(self, item, spider):` 方法，每个 item 生成后都会被这个方法处理。

### 启用一个Item Pipeline组件

```python
ITEM_PIPELINES = {
    'myproject.pipelines.PricePipeline': 300,
    'myproject.pipelines.JsonWriterPipeline': 800,
}
```

分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。


## Item Loaders



# Scrapy 中引入 Django Model

## 安装

```
pip3 install scrapy-djangoitem
```

## settings.py

```python
import sys

sys.path.append('/Users/double/Documents/djangoblog/blog')

from django.core.wsgi import get_wsgi_application

application = get_wsgi_application()

os.environ['DJANGO_SETTINGS_MODULE'] = 'config.settings'

import django

django.setup()
```

## 可能需要添加环境变量
```
export PYTHONPATH=$PYTHONPATH:/Users/double/Documents/djangoblog/blog
```

## Item 修改

```python
from blog.models import Quanzhifashi

class StoryDjangoItem(DjangoItem):
    django_model = Quanzhifashi

```

## Spider 中创建 Item
```python
    def parse(self, response):
        selector = Selector(response)
        selectors = selector.xpath("//a[@rel]")
        for s in selectors:
            story = StoryDjangoItem()
            story['url'] = s.xpath("@href").extract()[0].strip()
            story['name'] = s.xpath("text()").extract()[0].strip()
            yield story
```

## Pipeline 中保存数据

```python
class StoryPipeline(object):

    def process_item(self, item, spider):
        item.save()
        return item
```

## 多次请求生成一个 item
例子：

```python
class DoubanSpider(scrapy.Spider):
    name = "douban"
    # allowed_domains = ["douban.com"]  # https://movie.douban.com/top250
    start_urls = [
        "http://www.quanzhifashi.com/",
        # "https://movie.douban.com/top250",
    ]

    def parse(self, response):
        selector = Selector(response)
        selectors = selector.xpath("//a[@rel]")
        for s in selectors:
            story_item = StoryDjangoItem()
            story_item['url'] = s.xpath("@href").extract()[0].strip()
            story_item['name'] = s.xpath("text()").extract()[0].strip()

            request = Request(story_item['url'], callback=self.parse_detail, meta={'item': story_item})
            yield request

            # return story_item


    def parse_detail(self, response):
        story_item = response.meta['item']
        story_item['body'] = ''.join(response.selector.xpath("//div[@class='entry']/node()").extract())
        return story_item
```

## 报错记录

### ImproperlyConfigured

```
django.core.exceptions.ImproperlyConfigured: Requested setting LOGGING_CONFIG, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.
```

解决：

```
export DJANGO_SETTINGS_MODULE=mysite.settings
```

# 下面是文档记录

# Spider.class

定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。

爬取步骤：

1. 以初始的URL初始化Request，并设置回调函数。 当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。
2. 在回调函数内分析返回的(网页)内容，返回 Item 对象、dict、 Request 或者一个包括三者的可迭代容器。 返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。
3. 在回调函数内，您可以使用 选择器(Selectors) (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。
4. 最后，由spider返回的item将被存到数据库(由某些 Item Pipeline 处理)或使用 Feed exports 存入到文件中。

## scrapy.Spider

- name（唯一）
    定义spider名字的字符串(string)。通常以要爬取的网站（domain）作为名称
- allowed_domains
    可选。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时， 域名不在列表中的URL不会被跟进。
- start_urls：URL列表。
- custom_settings
- crawler
- settings
- logger
- from_crawler(crawler, *args, **kwargs)
- start_requests()
- make_requests_from_url(url)
- parse(response)
- log(message[, level, component])
- closed(reason)

## Generic Spiders

### CrawlSpider

分页

```python
# -*- coding: utf-8 -*-
from pyquery import PyQuery as pq
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor


class SegSpider(CrawlSpider):
    name = "seg"
    allowed_domains = ["segmentfault.com"]
    start_urls = (
        'http://segmentfault.com/t/html5?type=newest&page=1',
    )

    rules = (
        Rule(SgmlLinkExtractor(allow=('\/t\/html5\?type=newest\&page=\d', ))),
        Rule(SgmlLinkExtractor(allow=('\/q\/\d+', )), callback='parse_item'),
    )

    def parse_item(self, response):
        html = response.body
        v = pq(html)
        item = dict()
        item['url'] = response.url
        item['title'] = v('title').text()
        yield item
```

### XMLFeedSpider

### CSVFeedSpider

### SitemapSpider

#  Selectors 选择器

常用：

- BeautifulSoup
- lxml 

Scrapy: XPath (Scrapy选择器构建于 lxml 库之上)

## XPath

基本语法：

- `/html/head/title`:  选择HTML文档中 `<head>` 标签内的 `<title>` 元素
- `/html/head/title/text()`: 选择 `<title>` 元素内的文本
- `//td`: 选择所有的 `<td>` 元素
- `//div[@class="mine"]`: 选择所有具有 `class="mine"` 属性的 div 元素

为了方便使用 XPaths，Scrapy 提供 Selector 类， 有四种方法 :

- xpath(): 返回selectors列表, 每一个selector表示一个xpath参数表达式选择的节点.
- css(): 返回selectors列表, 每一个selector表示CSS参数表达式选择的节点
- extract(): 返回一个unicode字符串，该字符串为XPath选择器返回的数据
- re(): 返回unicode字符串列表，字符串作为参数由正则表达式提取出来

#### 术语

##### 节点 Node
七种类型

1. 元素 element
2. 属性 Attribute
3. 文本 text
4. 命名空间 namespace
5. 处理指令
6. 注释
7. 文档 （根）节点

##### 基本值 Atomic value 原子值

没有父节点或者无子节点

##### 项目 Item

项目是基本值或者节点

#### 节点关系

- Parent
- Children
- Sibling
- Ancestor
- Descendant

#### 语法

#### 选取节点

- `nodename`: 选取此节点的所有子节点
- `/`: 从根节点选取
- `//`: 从匹配旋转的当前节点选择文档中的节点，而不考虑它们的位置
- `.`: 选取当前节点
- `..` : 选取当前节点的父节点
- `@` : 选取属性

#### 谓语（Predicates）

谓语用来查找某个特定的节点或者包含某个指定的值的节点。

谓语被嵌在方括号中。

#### 选取未知节点

- `*`: 匹配任何元素节点
- `@*`：匹配任何属性节点
- `node()`：匹配任何类型的节点

#### 选取若干路径

通过在路径表达式中使用“|”运算符，您可以选取若干个路径。

ex:
```
//title | //price   选取文档中的所有 title 和 price 元素。
```

#### 轴 Axes

轴可定义相对于当前节点的节点集。

- ancestor: 选取当前节点的所有先辈（父、祖父等）。
- ancestor-or-self: 选取当前节点的所有先辈（父、祖父等）以及当前节点本身。
- attribute: 选取当前节点的所有属性。
- child: 选取当前节点的所有子元素。
- descendant: 选取当前节点的所有后代元素（子、孙等）。
- descendant-or-self: 选取当前节点的所有后代元素（子、孙等）以及当前节点本身。
- following: 选取文档中当前节点的结束标签之后的所有节点。
- namespace: 选取当前节点的所有命名空间节点。
- parent: 选取当前节点的父节点。
- preceding: 选取文档中当前节点的开始标签之前的所有节点。
- preceding-sibling: 选取当前节点之前的所有同级节点。
- self: 选取当前节点。

#### 位置路径表达式

位置路径可以是绝对的，也可以是相对的。

绝对路径起始于正斜杠( / )，而相对路径不会这样。在两种情况中，位置路径均包括一个或多个步，每个步均被斜杠分割：


绝对位置路径：
```
/step/step/...
```

相对位置路径：
```
step/step/...
```

step 包括：
- axis 轴 
- node-test 节点测试
- predicate 零个或者更多谓语

step 语法：

```
轴名称::节点测试[谓语]
```

例子：

- child::book 选取所有属于当前节点的子元素的 book 节点。
- attribute::lang 选取当前节点的 lang 属性。
- child::*    选取当前节点的所有子元素。
- attribute::*    选取当前节点的所有属性。
- child::text()   选取当前节点的所有文本子节点。
- child::node()   选取当前节点的所有子节点。
- descendant::book    选取当前节点的所有 book 后代。
- ancestor::book  选择当前节点的所有 book 先辈。
- ancestor-or-self::book  选取当前节点的所有 book 先辈以及当前节点（如果此节点是 book 节点）
- child::*/child::price   选取当前节点的所有 price 孙节点。

#### XPath 运算符




#### 运算符



# Items
爬取的主要目标就是从非结构性的数据源提取结构性数据

Item 对象是一种简单的容器，用于保存爬取到的数据。

## 声明 Item

Item 使用简单的 class 定义语法以及 Field 对象来声明。

例子：

```python
import scrapy

class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    stock = scrapy.Field()
    last_updated = scrapy.Field(serializer=str)
```


## Item Fields （字段）

Field 对象指明了每个字段的元数据 (metadata).

### Item 使用

#### create

```
>>> product = Product(name='Desktop PC', price=1000)
>>> print product
Product(name='Desktop PC', price=1000)
```

#### 获取对象中的 Field
```
>>> product['name']
Desktop PC
>>> product.get('name')
Desktop PC

>>> product['price']
1000

>>> product['last_updated']
Traceback (most recent call last):
    ...
KeyError: 'last_updated'
```

获取的时候设置 default 值：
```
>>> product.get('last_updated', 'not set')
not set
```

```
>>> product['lala'] # getting unknown field
Traceback (most recent call last):
    ...
KeyError: 'lala'
```

判断 Field 是否在 Item 中：
```
>>> 'name' in product  # is name field populated?
True

>>> 'last_updated' in product  # is last_updated populated?
False

>>> 'last_updated' in product.fields  # is last_updated a declared field?
True

>>> 'lala' in product.fields  # is lala a declared field?
False
```

#### 设置字段的值

```
>>> product['last_updated'] = 'today'
>>> product['last_updated']
today

>>> product['lala'] = 'test' # setting unknown field
Traceback (most recent call last):
    ...
KeyError: 'Product does not support field: lala'
```

#### 获取所有的值

使用 dict API：

```
>>> product.keys()
['price', 'name']

>>> product.items()
[('price', 1000), ('name', 'Desktop PC')]
```


#### 其他操作

复制item:

```
>>> product2 = Product(product)
>>> print product2
Product(name='Desktop PC', price=1000)

>>> product3 = product2.copy()
>>> print product3
Product(name='Desktop PC', price=1000)
```

根据item创建字典(dict):
```
>>> dict(product) # create a dict from all populated values
{'price': 1000, 'name': 'Desktop PC'}
```

根据字典(dict)创建item:
```
>>> Product({'name': 'Laptop PC', 'price': 1500})
Product(price=1500, name='Laptop PC')

>>> Product({'name': 'Laptop PC', 'lala': 1500}) # warning: unknown field in dict
Traceback (most recent call last):
    ...
KeyError: 'Product does not support field: lala'
```

# Item Loaders

 Items 提供了盛装抓取到的数据的 *容器* , 而Item Loaders提供了构件 *装载populating* 该容器。

## 用Item Loaders装载Items


例子：
```python
from scrapy.loader import ItemLoader
from myproject.items import Product

def parse(self, response):
    l = ItemLoader(item=Product(), response=response)
    l.add_xpath('name', '//div[@class="product_name"]')
    l.add_xpath('name', '//div[@class="product_title"]')
    l.add_xpath('price', '//p[@id="price"]')
    l.add_css('stock', 'p#stock]')
    l.add_value('last_updated', 'today') # you can also use literal values
    return l.load_item()
```

## Input and Output processors

Item Loader 在每个 (Item) 字段中都包含了一个输入处理器和一个输出处理器｡

输入处理器收到数据时立刻提取数据 (通过 add_xpath(), add_css() 或者 add_value() 方法) 之后输入处理器的结果被收集起来并且保存在 ItemLoader 内. 收集到所有的数据后, 调用 ItemLoader.load_item() 方法来填充, 并得到填充后的 Item 对象. 这是当输出处理器被和之前收集到的数据(和用输入处理器处理的)被调用.输出处理器的结果是被分配到Item的最终值｡

例子：
```python
l = ItemLoader(Product(), some_selector)
l.add_xpath('name', xpath1) # (1)
l.add_xpath('name', xpath2) # (2)
l.add_css('name', css) # (3)
l.add_value('name', 'test') # (4)
return l.load_item() # (5)
```

执行的流程：

1. 从 xpath1 提取出的数据,传递给 输入处理器 的 name 字段.输入处理器的结果被收集和保存在Item Loader中(但尚未分配给该Item)｡
2. 从 xpath2 提取出来的数据,传递给(1)中使用的相同的 输入处理器 .输入处理器的结果被附加到在(1)中收集的数据(如果有的话) ｡
3. This case is similar to the previous ones, except that the data is extracted from the css CSS selector, and passed through the same input processor used in (1) and (2). The result of the input processor is appended to the data collected in (1) and (2) (if any).
4. This case is also similar to the previous ones, except that the value to be collected is assigned directly, instead of being extracted from a XPath expression or a CSS selector. However, the value is still passed through the input processors. In this case, since the value is not iterable it is converted to an iterable of a single element before passing it to the input processor, because input processor always receive iterables.
5. The data collected in steps (1), (2), (3) and (4) is passed through the output processor of the name field. The result of the output processor is the value assigned to the name field in the item.




# Scrapy shell

通常使用这个终端来测试XPath或CSS表达式

推荐使用 IPython

## 启动 shell

```
scrapy shell <url>
```

## 使用终端

shortcut:

- shekp() : 打印可用对象及快捷命令的帮助列表
- fetch(request_or_url) : 根据给定的请求(request)或URL获取一个新的response，并更新相关的对象
- view(response) : 在本机的浏览器打开给定的response。 其会在response的body中添加一个 <base> tag ，使得外部链接(例如图片及css)能正确显示。 注意，该操作会在本地创建一个临时文件，且该文件不会被自动删除。

## 可用的 Scrapy 对象

Scrapy终端根据下载的页面会自动创建一些方便使用的对象

- crawler
- spider: 处理URL的spider
- request: 最近获取到的页面的 Request 对象
- response: 包含最近获取到的页面的 Response 对象。
- sel: 根据最近获取到的response构建的 Selector 对象。
- settings: 当前的 Scrapy settings

## 终端会话(shell session)样例

```
scrapy shell 'http://scrapy.org' --nolog
```

接着该终端(使用Scrapy下载器(downloader))获取URL内容并打印可用的对象及快捷命令(注意到以 [s] 开头的行):

```
[s] Available Scrapy objects:
[s]   crawler    <scrapy.crawler.Crawler object at 0x1e16b50>
[s]   item       {}
[s]   request    <GET http://scrapy.org>
[s]   response   <200 http://scrapy.org>
[s]   sel        <Selector xpath=None data=u'<html>\n  <head>\n    <meta charset="utf-8'>
[s]   settings   <scrapy.settings.Settings object at 0x2bfd650>
[s]   spider     <Spider 'default' at 0x20c6f50>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

>>>
```

操作这些对象：
```
>>> sel.xpath("//h2/text()").extract()[0]
u'Welcome to Scrapy'

>>> fetch("http://slashdot.org")
[s] Available Scrapy objects:
[s]   crawler    <scrapy.crawler.Crawler object at 0x1a13b50>
[s]   item       {}
[s]   request    <GET http://slashdot.org>
[s]   response   <200 http://slashdot.org>
[s]   sel        <Selector xpath=None data=u'<html lang="en">\n<head>\n\n\n\n\n<script id="'>
[s]   settings   <scrapy.settings.Settings object at 0x2bfd650>
[s]   spider     <Spider 'default' at 0x20c6f50>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

>>> sel.xpath('//title/text()').extract()
[u'Slashdot: News for nerds, stuff that matters']

>>> request = request.replace(method="POST")

>>> fetch(request)
[s] Available Scrapy objects:
[s]   crawler    <scrapy.crawler.Crawler object at 0x1e16b50>
...

>>>
```

## 在spider中启动shell来查看response


# Item Pipeline

作用：

- 清理  HTML 数据
- 验证爬取的数据 （检查 item 包含某些字段）
- 查重 （并丢弃）
- 将爬取结果保存到数据库中


## 编写 Item Pipeline

每个 Item Pipiline 组件是一个独立的 Python 类，同时必须实现下面的方法：

- process_item(self, item, spider)
    每个 item pipeline 组件都需要调用该方法，这个方法必须返回一个具有数据的dict，或是 Item (或任何继承类)对象， 或是抛出 DropItem 异常，被丢弃的item将不会被之后的pipeline组件所处理。

其他方法

- open_spider(self, spider)：当spider被开启时，这个方法被调用。
- close_spider(self, spider)：当spider被关闭时，这个方法被调用
- from_crawler(cls, crawler)

## Item pipeline 样例

### 验证价格，同时丢弃没有价格的item

```python
from scrapy.exceptions import DropItem

class PricePipeline(object):

    vat_factor = 1.15

    def process_item(self, item, spider):
        if item['price']:
            if item['price_excludes_vat']:
                item['price'] = item['price'] * self.vat_factor
            return item
        else:
            raise DropItem("Missing price in %s" % item)
```


### 将item写入JSON文件
以下pipeline将所有(从所有spider中)爬取到的item，存储到一个独立地 items.jl 文件，每行包含一个序列化为JSON格式的item:

```python
import json

class JsonWriterPipeline(object):

    def __init__(self):
        self.file = open('items.jl', 'wb')

    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + "\n"
        self.file.write(line)
        return item
```

### Write items to MongoDB

```python
import pymongo

class MongoPipeline(object):

    collection_name = 'scrapy_items'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        self.db[self.collection_name].insert(dict(item))
        return item
```

## 去重
```python
from scrapy.exceptions import DropItem

class DuplicatesPipeline(object):

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        if item['id'] in self.ids_seen:
            raise DropItem("Duplicate item found: %s" % item)
        else:
            self.ids_seen.add(item['id'])
            return item
```

## 启用一个Item Pipeline组件

```python
ITEM_PIPELINES = {
    'myproject.pipelines.PricePipeline': 300,
    'myproject.pipelines.JsonWriterPipeline': 800,
}
```

分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。


# Feed exports

## 序列化方式(Serialization formats)

feed输出使用到了 Item exporters 。其自带支持的类型有:

- JSON
- JSON lines
- CSV
- XML

同时可以通过设置 `FEED_EXPORTERS` 扩展格式

## JSON

- FEED_FORMAT：
- Exporter used: JsonItemExporter
- warning:
    JSON is very simple and flexible serialization format, but it doesn’t scale well for large amounts of data since incremental (aka. stream-mode) parsing is not well supported (if at all) among JSON parsers (on any language), and most of them just parse the entire object in memory. If you want the power and simplicity of JSON with a more stream-friendly format, consider using JsonLinesItemExporter instead, or splitting the output in multiple chunks.

## CSV

- FEED_FORMAT: csv
- Exporter used: CsvItemExporter
- FFED_EXPORT_FIELDS: 定义 CSV 的列。

## XML

- FEED_FORMAT: xml
- Exporter used: XmlItemexporter

## Pickle:

- FEED_FORMAT: pickle
- Exporter used: PickleItemExporter

## Marshal

- FEED_FORMAT: marshal
- Exporter used: MarshalItemExporter

## Storages

使用 URI (FEED_URI) 设置存储的位置，支持多个存储位置

存储设置支持：

- Local filesystem
- FTP
- S3
- Standard output

### Storage URI parameters

URI 可以动态生成，使用下面的参数：

- `%(time)s` - 被 timestamp 代替当 feed 创建的时候
- `%(name)s` - 被 spider name 代替

- `%(site_id)s` - 被 spider.site_id 代替

URI 例子：

- ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json
- s3://mybucket/scraping/feeds/%(name)s/%(time)s.json

### Storage backends

#### Local filesystem

- URI scheme: `file`
- Example URI: `file:///temp/export.csv`
- Required external libraries: one

#### FTP

- URI scheme: `ftp`
- Example URI: `ftp://user:pass@ftp.example.com/path/to/export.csv`
- Required external libraries: one

#### S3

#### Standard output


## Settings

- FEED_URI
- FEEED_FORMAT
- FEED_STORAGES
- FEED_STORE_EMPTY
- FEED_EXPORT_ENCODING
- FEED_EXPORT_FIELDS


# Requests and Responses


## Request objects

```python
class scrapy.http.Request(url[, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback])
```

## Response objects

```python
class scrapy.http.Response(url[, status=200, headers=None, body=b'', flags=None, request=None])
```

获取 Selector： `response.selector`


# Link Extractors

```python
scrapy.linkextractors import LinkExtractor
```

Method: `extract_links` 接收一个 Response objecy , returns a list of `scrapy.link.Link`

它在 `CrawlSpider` 类中使用

### LxmlLinkExtractor

# Settings

# Exceptions

## Built-in Exceptions reference

### DropItem

Item Pipeline

### CloseSpider

### IgnoreRequest

### NotConfigured

### NotSupported

# Built-in services

## Logging

```python
import logging
logging.warning("This is a warning")

import logging
logging.log(logging.WARNING, "This is a warning")
```

### Logging from Spiders

Spider 类中提供了 `logger`:

```python
import scrapy

class MySpider(scrapy.Spider):

    name = 'myspider'
    start_urls = ['http://scrapinghub.com']

    def parse(self, response):
        self.logger.info('Parse function called on %s', response.url)
```

### Logging configuration

#### Logging settings

- LOG_FILE
- LOG_ENABLED
- LOG_ENCODING
- LOG_LEVEL：定义最小现实的 log 级别
- LOG_FORMAT
- LOG_DATEFORMAT
- LOG_STDOUT

前两个设置定义了 log 信息的目的地。


#### Command-line options

- `--logfile FILE`
- `--loglevel/-L LEVEL`
- `--nolog`

## Stats Collection


### Common Stats Collector uses

获取 stats collector 通过 `stats` 属性：

```python
class ExtensionThatAccessStats(object):

    def __init__(self, stats):
        self.stats = stats

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.stats)
```

设置一个 stat 值：

```python
stats.set_value('hostname', socket.gethostname())
```

增加一个 stat 值：

```python
stats.inc_value('custom_count')
```

当大于原来的值时设置值：
```python
stats.mac_value('max_items_scraped', value)
```

当小于原来的值时设置值：
```python
stats.min_value('min_free_memory_percent', value)
```

获取 stat 值：
```python
>>>stats.get_value('custom_count')
1
```

获取所有 stat：
```python
>>> stats.get_stats()
{'custom_count': 1, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}
```

### Available Stats Collectors

#### MemoryStatsCollector

#### DummyStatsCollector

## Sending e-mail

via `smtplib`

### Quick example

构造方法：

构造方法 - 标准：

```python
from scrapy.mail import MailSender
mailer = MailSender()
```

构造方法 - Scrapy settings：

```python
mailer = MailSender.from_settings(settings)
```

usage:
```python
mailer.send(to=["someone@example.com"], subject="Some subject", body="Some body", cc=["another@example.com"])
```

### MailSender class reference

### Mail settings

在构造方法中设置

#### MAIL_FROM

#### MAIL_HOST

#### MAIL_PORT

Default: `25`

#### MAIL_USER

#### MAIL_PASS

#### MAIL_TLS

#### MAIL_SSL

## Telnet Console

# Solving specific problems


## Jobs：pausing and resuming crawls

Scrapy 支持停止和继续爬虫，不过需要提供下面的内容：

- scheduler 调取器，保存到磁盘中的
- 一个把请求保存到磁盘的 副本过滤器 (duplicates filter)
- 一个能够保存爬虫状态的扩展 （key/value 形式)

### Job directory

通过 `JOBDIR` 设置 *job directory*, 这个文件夹会被用来保存 job 的状态，这个文件夹会被多个爬虫共用

### How to use it

开启一个可持续化的爬虫：

```
scrapy crawl somespider -s JOBDIR=crawls/somespider-1
```

然后这个爬虫可以被认为或者被动的停止，通过下面的命令回复爬虫：

```
scrapy crawl simespider -s JOBDIRE=crawls/somespider-1
```

### Keeping persistent state between batches

有时候需要保存爬虫在运行过程中的状态，使用`spider.state`: 下面保存 item 的数量

```python
def parse_item(self, response):
    # parse item here
    self.state['items_count'] = self.state.get('item_count', 0) + 1
```








参考：

- [http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/spiders.html](http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/spiders.html)